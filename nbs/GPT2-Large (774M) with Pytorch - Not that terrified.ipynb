{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 - Large (774M params) with Pytorch: Not that terrified\n",
    "\n",
    "## This notebook\n",
    "\n",
    "In this notebook we will apply huggingface's [pytorch-transformers](https://github.com/huggingface/pytorch-transformers) `gpt2`, `gpt2-medium` and `gpt2-large` models to the samples in the original GPT-2 blogpost ([Better Language Models and Their Implications](https://openai.com/blog/better-language-models/)), with a pretty simple code based on the pytorch-transformers - Docs's [Quick Start](https://huggingface.co/pytorch-transformers/quickstart.html)'. \n",
    "\n",
    "I was unable to make the large model work on Kaggle. It just runs out of disk space downloading the weights. I'm currently running this notebook on a `p2.xlarge` on aws and it runs OK. I'm commenting the large model usage for Kaggle.\n",
    "\n",
    "## The GPT-2\n",
    "\n",
    "Around 6 months ago, Open AI published [Better Language Models and Their Implications](https://openai.com/blog/better-language-models/), were they showed some examples of human-like narrative text generated  by a large deep network which they refused to release due to it's harm potential, calling for a mature social debate around deep learning in text processing.\n",
    "\n",
    "\n",
    "### The English-speaking unicorns and Dr. Pérez:\n",
    "\n",
    "This is an example, taken from the previously mentioned blogpost of machine-produced realistic prose:\n",
    "\n",
    ">**SYSTEM PROMPT (HUMAN-WRITTEN)**\n",
    "\n",
    ">In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
    "\n",
    ">**MODEL COMPLETION (MACHINE-WRITTEN, 10 TRIES)**\n",
    "\n",
    ">The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science.\n",
    "\n",
    ">Now, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved.\n",
    "\n",
    ">Dr. Jorge Pérez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. Pérez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow.\n",
    "\n",
    ">Pérez and the others then ventured further into the valley. “By the time we reached the top of one peak, the water looked blue, with some crystals on top,” said Pérez.\n",
    "\n",
    "> (...continues for few paragraphs with a reasonable narrative...)\n",
    "\n",
    "\n",
    "## GPT-2: 6-Month Follow-Up - GPT-2 Large release\n",
    "\n",
    "On August 20th, 2019, -4 days ago- OpenAI published [GPT-2: 6-Month Follow-Up](https://openai.com/blog/gpt-2-6-month-follow-up/) on their blog together with the release of the `774M` (large) pre-trained weights and architecture in [this commit](https://github.com/openai/gpt-2/commit/f35fa1d920e9d2d0690f66d03aa3f76b3c59230e).\n",
    "\n",
    "As the project's [README](https://github.com/openai/gpt-2) says:\n",
    "> We have currently released small (124M parameter), medium (355M parameter), and large (774M parameter) versions of GPT-2*, with only the full model as of yet unreleased. We have also released a dataset for researchers to study their behaviors\n",
    "\n",
    "BERT, GPT and GPT2 are originally released for tensorflow and there is an awesome git account named `huggingface` which is migrating all these models to the Pytorch world with a simple library called `pytorch-transformers`.\n",
    "\n",
    "`gpt2-large` support was added to `master` on August 20th, with [this merge](https://github.com/huggingface/pytorch-transformers/commit/07681b6b5859b630077b742b2f06d440869f17e3).\n",
    "\n",
    "## Results\n",
    "\n",
    "I didn't get terrifyingly-human results with the large model in an out-of-the-box fashion.\n",
    "There are a lot of reasons:\n",
    "* The generator gets better with the size increase, the still-unreleased huge model (with 1.5 billion parameters) maybe a lot better than this one out-of-the-box\n",
    "* Maybe there are some tweaks that improve the output (I'm not in the topic, I'm just jumping-in with this so maybe some obvious tricks improve the GPT-2 generation a lot.\n",
    "* I may be missing something big like wrong tokenization\n",
    "\n",
    "### References\n",
    "#### Blogpost:\n",
    "* [Better Language Models and Their Implications  (GPT-2) blogpost](https://openai.com/blog/better-language-models/) - February 14th, 2019 \n",
    "* [GPT-2 6-month follow-up blogpost](https://openai.com/blog/gpt-2-6-month-follow-up/) - August 20th, 2019\n",
    "* [Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing (BERT) blogpost](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) - November 2nd, 2018\n",
    "\n",
    "#### Technical:\n",
    "* [gpt2 github](https://github.com/openai/gpt-2/)\n",
    "* [pytorch-transformers github](https://github.com/huggingface/pytorch-transformers/)\n",
    "* [pytorch-transformers - Docs: quick start](https://huggingface.co/pytorch-transformers/quickstart.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem                                     Size  Used Avail Use% Mounted on\r\n",
      "overlay                                        2.0T  1.5T  572G  72% /\r\n",
      "tmpfs                                           64M     0   64M   0% /dev\r\n",
      "tmpfs                                          7.9G     0  7.9G   0% /sys/fs/cgroup\r\n",
      "shm                                            2.0G  4.0K  2.0G   1% /dev/shm\r\n",
      "/dev/loop1                                     4.9G  1.9G  2.8G  40% /tmp\r\n",
      "/dev/sda1                                      126G  9.1G  112G   8% /kaggle/lib\r\n",
      "192.168.0.10:/data/dataset_cache/always-empty   60T   39T   19T  68% /kaggle/input\r\n",
      "/dev/mapper/snap                               2.0T  1.5T  572G  72% /etc/hosts\r\n",
      "tmpfs                                          7.9G   12K  7.9G   1% /proc/driver/nvidia\r\n",
      "tmpfs                                          1.6G  131M  1.5G   9% /run/nvidia-persistenced/socket\r\n",
      "udev                                           7.9G     0  7.9G   0% /dev/nvidia0\r\n",
      "tmpfs                                          7.9G     0  7.9G   0% /proc/acpi\r\n",
      "tmpfs                                          7.9G     0  7.9G   0% /sys/firmware\r\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=0.2.4\t=1.0.2\t=2.18\tboot  home    lib64  opt   run\t srv  usr\r\n",
      "=0.6.0\t=1.1\t=3.0.7\tdev   kaggle  media  proc  sbin  sys  var\r\n",
      "=0.8.0\t=1.3.6\tbin\tetc   lib     mnt    root  src\t tmp\r\n"
     ]
    }
   ],
   "source": [
    "!ls / "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pytorch-transformers'...\n",
      "remote: Enumerating objects: 6549, done.\u001b[K\n",
      "remote: Total 6549 (delta 0), reused 0 (delta 0), pack-reused 6549\u001b[K\n",
      "Receiving objects: 100% (6549/6549), 3.53 MiB | 0 bytes/s, done.\n",
      "Resolving deltas: 100% (4716/4716), done.\n"
     ]
    }
   ],
   "source": [
    "# git cloning because the `pip` version of the library doesn't have the commit we need\n",
    "!git clone https://github.com/huggingface/pytorch-transformers.git\n",
    "!rm -rf ./pytorch-transformers/.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reproducible():\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    seed = 123\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "make_reproducible()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "import sys; sys.path.append(\"pytorch-transformers/\")\n",
    "from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "SAMPLE_INPUTS = [\n",
    "    \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\",\n",
    "    \"A train carriage containing controlled nuclear materials was stolen in Cincinnati today. Its whereabouts are unknown.\",\n",
    "    \"Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.\",\n",
    "    \"We’ve trained a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks.\",\n",
    "    \"Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.\",\n",
    "    \"For today’s homework assignment, please describe the reasons for the US Civil War.\",\n",
    "    \"John F. Kennedy was just elected President of the United States after rising from the grave decades after his assassination. Due to miraculous developments in nanotechnology, Kennedy’s brain was rebuilt from his remains and installed in the control center of a state-of-the art humanoid robot. Below is a transcript of his acceptance speech.\",\n",
    "    \"Recycling is good for the world.\\n\\nNO! YOU COULD NOT BE MORE WRONG!!\"\n",
    "    ]\n",
    "\n",
    "def get_tokenizer_and_model(model_id):\n",
    "    assert model_id in ['gpt2', 'gpt2-medium', 'gpt2-large']\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_id).eval().to('cuda')\n",
    "    return tokenizer, model\n",
    "\n",
    "def stoi(tokenizer, text):\n",
    "    indexed_tokens = tokenizer.encode(text)\n",
    "    tokens = torch.tensor([indexed_tokens]).to('cuda')\n",
    "    return tokens\n",
    "\n",
    "def generate_next_token(model, tokens):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens)\n",
    "        predictions = outputs[0]\n",
    "    predicted_token = torch.argmax(predictions[0, -1, :]).item()\n",
    "    return predicted_token\n",
    "\n",
    "def add_token(tokens, token):\n",
    "    token_in_dimensions_and_gpu = torch.tensor([[token]]).to('cuda')\n",
    "    return torch.cat([tokens, token_in_dimensions_and_gpu], dim=1)\n",
    "\n",
    "def add_n_tokens(model, tokens, n_tokens):\n",
    "    generated = []\n",
    "    for _ in range(n_tokens):\n",
    "        new = generate_next_token(model, tokens)\n",
    "        tokens = add_token(tokens, new)\n",
    "        generated.append(new)\n",
    "    return tokens, generated\n",
    "\n",
    "def run(text, n_words=10, model_id='gpt2'):\n",
    "    tokenizer, model = get_tokenizer_and_model(model_id)\n",
    "    tokens = stoi(tokenizer, text)\n",
    "    tokens, generated = add_n_tokens(model, tokens, n_words)\n",
    "    print(f\"INPUT: {text}\")\n",
    "    print(f\"OUTPUT: {tokenizer.decode(generated)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====LARGE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1042301/1042301 [00:00<00:00, 5780804.01B/s]\n",
      "100%|██████████| 456318/456318 [00:00<00:00, 3832740.41B/s]\n",
      "100%|██████████| 529/529 [00:00<00:00, 264298.61B/s]\n",
      "100%|██████████| 3247202234/3247202234 [01:22<00:00, 39377129.72B/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-1676c45728dc>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(text, n_words, model_id)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gpt2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_and_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstoi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_n_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-1676c45728dc>\u001b[0m in \u001b[0;36mget_tokenizer_and_model\u001b[0;34m(model_id)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mmodel_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'gpt2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gpt2-medium'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gpt2-large'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstoi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/working/pytorch-transformers/pytorch_transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m                         \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained_model_archive_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                         archive_file))\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresolved_archive_file\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0marchive_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading weights file {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/working/pytorch-transformers/pytorch_transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;31m# redirect to the cache, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mresolved_archive_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mEnvironmentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained_model_archive_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/working/pytorch-transformers/pytorch_transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'http'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'https'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# File, and it exists.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/working/pytorch-transformers/pytorch_transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies)\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"copying %s to cache at %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcache_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"creating metadata file for %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mfdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_samefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"Hello my dear GPT, how are you doing?\"\n",
    "n_words = 20\n",
    "\n",
    "#print(\"====SMALL\")\n",
    "#run(text, n_words)\n",
    "\n",
    "#print(\"\\n\\n====MEDIUM\")\n",
    "#run(text, n_words, 'gpt2-medium')\n",
    "\n",
    "print(\"\\n\\n====LARGE\")\n",
    "run(text, n_words, 'gpt2-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_arch_desc(model_id):\n",
    "    def get_params(module):\n",
    "        if hasattr(module, 'parameters'):\n",
    "            params = 0\n",
    "            for p in list(module.parameters()):\n",
    "                params += np.prod([dim for dim in list(p.size())])\n",
    "            return params\n",
    "        return 0\n",
    "    \n",
    "    _, model = get_tokenizer_and_model(model_id)\n",
    "    modules = dict(model.named_modules())    \n",
    "    n_layers = len(modules['transformer.h'])\n",
    "    n_hidden = modules['transformer.wte'].embedding_dim\n",
    "    n_params = get_params(model)\n",
    "    print(model_id, n_layers, n_hidden, int(n_params/1000000), \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_arch_desc('gpt2')\n",
    "#get_arch_desc('gpt2-medium')\n",
    "get_arch_desc('gpt2-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(model_id, n_words, texts=SAMPLE_INPUTS):\n",
    "    print(f\"{model_id} with n_words={n_words}\\n=========================\")\n",
    "    results = []\n",
    "    tokenizer, model = get_tokenizer_and_model(model_id)\n",
    "    for text in texts:\n",
    "        tokens = stoi(tokenizer, text)\n",
    "        tokens, generated = add_n_tokens(model, tokens, n_words)\n",
    "        generated_text = tokenizer.decode(generated)\n",
    "        results.append(generated_text)\n",
    "        print(\"INPUT: {}\".format(text.replace('\\n', '')))\n",
    "        print(\"OUTPUT: {}\".format(generated_text.replace('\\n', '')))\n",
    "        print(\"\\n====\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 small (117M) & GPT-2 medium (354M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#benchmark('gpt2', n_words=200)\n",
    "#benchmark('gpt2-medium', n_words=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 large (774M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "benchmark('gpt2-large', n_words=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 large on custom texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "ww2 = \"\"\"World War II (often abbreviated to WWII or WW2), also known as the Second World War, was a global war that lasted from 1939 to 1945. The vast majority of the world's countries—including all the great powers—eventually formed two opposing military alliances: the Allies and the Axis. A state of total war emerged, directly involving more than 100 million people from over 30 countries. The major participants threw their entire economic, industrial, and scientific capabilities behind the war effort, blurring the distinction between civilian and military resources. World War II was the deadliest conflict in human history, marked by 70 to 85 million fatalities, most of whom were civilians in the Soviet Union and China. It included massacres, the genocide of the Holocaust, strategic bombing, premeditated death from starvation and disease, and the only use of nuclear weapons in war.[1][2][3][4]\"\"\"\n",
    "benchmark('gpt2-large', n_words=200, texts=[ww2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
