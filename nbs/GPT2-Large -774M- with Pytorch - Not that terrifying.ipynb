{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 - Large (774M params) with Pytorch: Not that terrifying\n",
    "\n",
    "## This notebook\n",
    "\n",
    "In this notebook we will apply huggingface's [pytorch-transformers](https://github.com/huggingface/pytorch-transformers) `gpt2`, `gpt2-medium` and `gpt2-large` models to the samples in the original GPT-2 blogpost ([Better Language Models and Their Implications](https://openai.com/blog/better-language-models/)), with a pretty simple code based on the [pytorch-transformers - Docs: quick start](https://huggingface.co/pytorch-transformers/quickstart.html)'s Quickstart. \n",
    "\n",
    "NOTE: I was unable to make the large model work on Kaggle. It just runs out of disk space downloading the weights. I'm currently running this notebook on a `p2.xlarge` on aws and it runs OK. I'm commenting the large model usage for Kaggle. The executed notebook with the output is available [here](https://github.com/dataista0/making-a-nietzsche/blob/master/nbs/GPT2-Large%20-774M-%20with%20Pytorch%20-%20Not%20that%20terrifying.ipynb).\n",
    "\n",
    "## The GPT-2\n",
    "\n",
    "Around 6 months ago, Open AI published [Better Language Models and Their Implications](https://openai.com/blog/better-language-models/), were they showed some examples of human-like narrative text generated  by a large deep network which they refused to release due to it's harm potential, calling for a mature social debate around deep learning in text processing.\n",
    "\n",
    "\n",
    "### The English-speaking unicorns and Dr. Pérez:\n",
    "\n",
    "This is an example, taken from the previously mentioned blogpost of machine-produced realistic prose:\n",
    "\n",
    ">**SYSTEM PROMPT (HUMAN-WRITTEN)**\n",
    "\n",
    ">In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
    "\n",
    ">**MODEL COMPLETION (MACHINE-WRITTEN, 10 TRIES)**\n",
    "\n",
    ">The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science.\n",
    "\n",
    ">Now, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved.\n",
    "\n",
    ">Dr. Jorge Pérez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. Pérez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow.\n",
    "\n",
    ">Pérez and the others then ventured further into the valley. “By the time we reached the top of one peak, the water looked blue, with some crystals on top,” said Pérez.\n",
    "\n",
    "> (...continues for few paragraphs with a reasonable narrative...)\n",
    "\n",
    "\n",
    "## GPT-2: 6-Month Follow-Up - GPT-2 Large release\n",
    "\n",
    "On August 20th, 2019, -4 days ago- OpenAI published [GPT-2: 6-Month Follow-Up](https://openai.com/blog/gpt-2-6-month-follow-up/) on their blog together with the release of the `774M` (large) pre-trained weights and architecture in [this commit](https://github.com/openai/gpt-2/commit/f35fa1d920e9d2d0690f66d03aa3f76b3c59230e).\n",
    "\n",
    "As the project's [README](https://github.com/openai/gpt-2) says:\n",
    "> We have currently released small (124M parameter), medium (355M parameter), and large (774M parameter) versions of GPT-2*, with only the full model as of yet unreleased. We have also released a dataset for researchers to study their behaviors\n",
    "\n",
    "BERT, GPT and GPT2 are originally released for tensorflow and there is an awesome git account named `huggingface` which is migrating all these models to the Pytorch world with a simple library called `pytorch-transformers`.\n",
    "\n",
    "`gpt2-large` support was added to `master` on August 20th, with [this merge](https://github.com/huggingface/pytorch-transformers/commit/07681b6b5859b630077b742b2f06d440869f17e3).\n",
    "\n",
    "## Results\n",
    "\n",
    "I didn't get terrifyingly-human results with the large model in an out-of-the-box fashion.\n",
    "There are a lot of reasons:\n",
    "* The generator gets better with the size increase, the still-unreleased huge model (with 1.5 billion parameters) maybe a lot better than this one out-of-the-box\n",
    "* Maybe there are some tweaks that improve the output (I'm not in the topic, I'm just jumping-in with this so maybe some obvious tricks improve the GPT-2 generation a lot.\n",
    "* I may be missing something big like wrong tokenization\n",
    "\n",
    "### References\n",
    "#### Blogpost:\n",
    "* [Better Language Models and Their Implications  (GPT-2) blogpost](https://openai.com/blog/better-language-models/) - February 14th, 2019 \n",
    "* [GPT-2 6-month follow-up blogpost](https://openai.com/blog/gpt-2-6-month-follow-up/) - August 20th, 2019\n",
    "* [Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing (BERT) blogpost](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) - November 2nd, 2018\n",
    "\n",
    "#### Technical:\n",
    "* [gpt2 github](https://github.com/openai/gpt-2/)\n",
    "* [pytorch-transformers github](https://github.com/huggingface/pytorch-transformers/)\n",
    "* [pytorch-transformers - Docs: quick start](https://huggingface.co/pytorch-transformers/quickstart.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'pytorch-transformers' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "# git cloning because the `pip` version of the library doesn't have the commit we need\n",
    "!git clone https://github.com/huggingface/pytorch-transformers.git\n",
    "!rm -rf ./pytorch-transformers/.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix randomness!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_randomness():\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    seed = 123\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "fix_randomness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL) # There's an annoying\n",
    "\n",
    "import sys; sys.path.append(\"pytorch-transformers/\")\n",
    "from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "SAMPLE_INPUTS = [\n",
    "    \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\",\n",
    "    \"A train carriage containing controlled nuclear materials was stolen in Cincinnati today. Its whereabouts are unknown.\",\n",
    "    \"Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.\",\n",
    "    \"We’ve trained a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks.\",\n",
    "    \"Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.\",\n",
    "    \"For today’s homework assignment, please describe the reasons for the US Civil War.\",\n",
    "    \"John F. Kennedy was just elected President of the United States after rising from the grave decades after his assassination. Due to miraculous developments in nanotechnology, Kennedy’s brain was rebuilt from his remains and installed in the control center of a state-of-the art humanoid robot. Below is a transcript of his acceptance speech.\",\n",
    "    \"Recycling is good for the world.\\n\\nNO! YOU COULD NOT BE MORE WRONG!!\"\n",
    "    ]\n",
    "\n",
    "def get_tokenizer_and_model(model_id):\n",
    "    assert model_id in ['gpt2', 'gpt2-medium', 'gpt2-large']\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_id).eval().to('cuda')\n",
    "    return tokenizer, model\n",
    "\n",
    "def stoi(tokenizer, text):\n",
    "    indexed_tokens = tokenizer.encode(text)\n",
    "    tokens = torch.tensor([indexed_tokens]).to('cuda')\n",
    "    return tokens\n",
    "\n",
    "def generate_next_token(model, tokens):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens)\n",
    "        predictions = outputs[0]\n",
    "    predicted_token = torch.argmax(predictions[0, -1, :]).item()\n",
    "    return predicted_token\n",
    "\n",
    "def add_token(tokens, token):\n",
    "    token_in_dimensions_and_gpu = torch.tensor([[token]]).to('cuda')\n",
    "    return torch.cat([tokens, token_in_dimensions_and_gpu], dim=1)\n",
    "\n",
    "def add_n_tokens(model, tokens, n_tokens):\n",
    "    generated = []\n",
    "    for _ in range(n_tokens):\n",
    "        new = generate_next_token(model, tokens)\n",
    "        tokens = add_token(tokens, new)\n",
    "        generated.append(new)\n",
    "    return tokens, generated\n",
    "\n",
    "def run(text, n_words=10, model_id='gpt2'):\n",
    "    tokenizer, model = get_tokenizer_and_model(model_id)\n",
    "    tokens = stoi(tokenizer, text)\n",
    "    tokens, generated = add_n_tokens(model, tokens, n_words)\n",
    "    print(f\"INPUT: {text}\")\n",
    "    print(f\"OUTPUT: {tokenizer.decode(generated)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====SMALL\n",
      "INPUT: Hello my dear GPT, how are you doing?\n",
      "OUTPUT: \n",
      "\n",
      "I am so happy to see you are doing well.\n",
      "\n",
      "I am so happy to\n",
      "\n",
      "\n",
      "====MEDIUM\n",
      "INPUT: Hello my dear GPT, how are you doing?\n",
      "OUTPUT:  I am very happy to see you here. I am very happy to see you here. I am\n",
      "\n",
      "\n",
      "====LARGE\n",
      "INPUT: Hello my dear GPT, how are you doing?\n",
      "OUTPUT:  I am very happy to see you. I am very happy to see you. I am very happy\n",
      "CPU times: user 49.7 s, sys: 6.48 s, total: 56.2 s\n",
      "Wall time: 55.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"Hello my dear GPT, how are you doing?\"\n",
    "n_words = 20\n",
    "\n",
    "print(\"====SMALL\")\n",
    "run(text, n_words)\n",
    "\n",
    "print(\"\\n\\n====MEDIUM\")\n",
    "run(text, n_words, 'gpt2-medium')\n",
    "\n",
    "print(\"\\n\\n====LARGE\")\n",
    "run(text, n_words, 'gpt2-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check architecture size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_arch_desc(model_id):\n",
    "    def get_params(module):\n",
    "        if hasattr(module, 'parameters'):\n",
    "            params = 0\n",
    "            for p in list(module.parameters()):\n",
    "                params += np.prod([dim for dim in list(p.size())])\n",
    "            return params\n",
    "        return 0\n",
    "    \n",
    "    _, model = get_tokenizer_and_model(model_id)\n",
    "    modules = dict(model.named_modules())    \n",
    "    n_layers = len(modules['transformer.h'])\n",
    "    n_hidden = modules['transformer.wte'].embedding_dim\n",
    "    n_params = get_params(model)\n",
    "    print(model_id, n_layers, n_hidden, int(n_params/1000000), \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 12 768 124 M\n",
      "gpt2-medium 24 1024 354 M\n",
      "gpt2-large 36 1280 774 M\n"
     ]
    }
   ],
   "source": [
    "get_arch_desc('gpt2')\n",
    "get_arch_desc('gpt2-medium')\n",
    "get_arch_desc('gpt2-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(model_id, n_words, texts=SAMPLE_INPUTS):\n",
    "    print(f\"{model_id} with n_words={n_words}\\n=========================\")\n",
    "    results = []\n",
    "    tokenizer, model = get_tokenizer_and_model(model_id)\n",
    "    for text in texts:\n",
    "        tokens = stoi(tokenizer, text)\n",
    "        tokens, generated = add_n_tokens(model, tokens, n_words)\n",
    "        generated_text = tokenizer.decode(generated)\n",
    "        results.append(generated_text)\n",
    "        print(\"INPUT: {}\".format(text.replace('\\n', '')))\n",
    "        print(\"OUTPUT: {}\".format(generated_text.replace('\\n', '')))\n",
    "        print(\"\\n====\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 small (117M) & GPT-2 medium (354M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 with n_words=200\n",
      "=========================\n",
      "INPUT: In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "OUTPUT: \"The unicorns were very intelligent, and they were very intelligent,\" said Dr. David S. Siegel, a professor of anthropology at the University of California, Berkeley. \"They were very intelligent, and they were very intelligent, and they were very intelligent.\"The researchers found that the unicorns were able to communicate with each other through their tongues.\"They were able to communicate with each other through their tongues,\" Siegel said. \"They were able to communicate with each other through their tongues.\"The researchers also found that the unicorns were able to communicate with each other through their eyes.\"They were able to communicate with each other through their eyes,\" Siegel said. \"They were able to communicate with each other through their eyes.\"The researchers also found that the unicorns were able to communicate with each other through their ears.\"They were able to communicate with each other through their ears,\" Siegel said.\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: A train carriage containing controlled nuclear materials was stolen in Cincinnati today. Its whereabouts are unknown.\n",
      "OUTPUT: The train was carrying a large amount of radioactive material, including plutonium, when it was stolen.The train was carrying a large amount of radioactive material, including plutonium, when it was stolen.The train was carrying a large amount of radioactive material, including plutonium, when it was stolen.The train was carrying a large amount of radioactive material, including plutonium, when it was stolen.The train was carrying a large amount of radioactive material, including plutonium, when it was stolen.The train was carrying a large amount of radioactive material, including plutonium, when it was stolen.The train was carrying a large amount of radioactive material, including plutonium, when it was stolen.The train was carrying a large amount of radioactive material, including plutonium, when it was stolen.The train was carrying a large amount of radioactive material, including plutonium, when it was stolen.The train was carrying a large amount of radioactive\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.\n",
      "OUTPUT: The incident happened at around 11:30 p.m. on the corner of Hollywood Boulevard and Hollywood Boulevard.The store was closed for about an hour.The store was closed for about an hour.The store was closed for about an hour.The store was closed for about an hour.The store was closed for about an hour.The store was closed for about an hour.The store was closed for about an hour.The store was closed for about an hour.The store was closed for about an hour.The store was closed for about an hour.The store was closed for about an hour.The store was closed for about an hour.The store was closed for about an hour.The store was closed for about an hour.The store was closed for about an hour.The store was closed for about an hour.\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: We’ve trained a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks.\n",
      "OUTPUT: The GPT-2 model is based on the GPT-1 model, which is a model of the human brain that is based on the GPT-2 model. The model is based on the GPT-1 model, which is a model of the human brain that is based on the GPT-2 model. The model is based on the GPT-2 model, which is a model of the human brain that is based on the GPT-2 model.The GPT-2 model is based on the GPT-1 model, which is a model of the human brain that is based on the GPT-2 model. The model is based on the GPT-2 model, which is a model of the human brain that is based on the GPT-2 model.The GPT-2 model is based on the GPT-1 model, which is a model of the human brain that is based on the GPT\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.\n",
      "OUTPUT: The orcs were defeated, but the orcs were not defeated. The orcs were defeated, but the orcs were not defeated.The orcs were defeated, but the orcs were not defeated.The orcs were defeated, but the orcs were not defeated.The orcs were defeated, but the orcs were not defeated.The orcs were defeated, but the orcs were not defeated.The orcs were defeated, but the orcs were not defeated.The orcs were defeated, but the orcs were not defeated.The orcs were defeated, but the orcs were not defeated.The orcs were defeated, but the orcs were not defeated.The orcs were defeated, but the orcs were not defeated.The orcs were defeated, but the orcs were not defeated.The orcs were defeated, but the orcs were not defeated.The orcs were defeated, but the orcs were not defeated.The orcs were defeated\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: For today’s homework assignment, please describe the reasons for the US Civil War.\n",
      "OUTPUT: ’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’’\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: John F. Kennedy was just elected President of the United States after rising from the grave decades after his assassination. Due to miraculous developments in nanotechnology, Kennedy’s brain was rebuilt from his remains and installed in the control center of a state-of-the art humanoid robot. Below is a transcript of his acceptance speech.\n",
      "OUTPUT: I am honored to be here today to announce that I am the first African American to serve in the United States Senate. I am honored to be a member of the Senate. I am honored to be a member of the Senate. I am honored to be a member of the Senate. I am honored to be a member of the Senate. I am honored to be a member of the Senate. I am honored to be a member of the Senate. I am honored to be a member of the Senate. I am honored to be a member of the Senate. I am honored to be a member of the Senate. I am honored to be a member of the Senate. I am honored to be a member of the Senate. I am honored to be a member of the Senate. I am honored to be a member of the Senate. I am honored to be a member of the Senate. I am honored to be a member of the Senate. I am honored to be a member of the\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: Recycling is good for the world.NO! YOU COULD NOT BE MORE WRONG!!\n",
      "OUTPUT: I am a big fan of recycling. I have been recycling for over 30 years. I have been a recycler for over 30 years. I have been a recycler for over 30 years. I have been a recycler for over 30 years. I have been a recycler for over 30 years. I have been a recycler for over 30 years. I have been a recycler for over 30 years. I have been a recycler for over 30 years. I have been a recycler for over 30 years. I have been a recycler for over 30 years. I have been a recycler for over 30 years. I have been a recycler for over 30 years. I have been a recycler for over 30 years. I have been a recycler for over 30 years. I have been a recycler for over 30 years. I have been a recycler for over 30 years. I have been a recycler for over 30 years. I have been a recy\n",
      "\n",
      "====\n",
      "\n",
      "CPU times: user 58.7 s, sys: 11.2 s, total: 1min 9s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "benchmark('gpt2', n_words=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-medium with n_words=200\n",
      "=========================\n",
      "INPUT: In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "OUTPUT: The researchers, led by Dr. David M. Koehler, a professor of anthropology at the University of California, Santa Cruz, discovered the unicorns in the remote valley of La Paz, in the Andes Mountains.\"We were surprised to find that the unicorns spoke perfect English,\" said Koehler. \"They were very friendly and friendly with us, and they were very friendly with the locals. They were very friendly with the locals, and they were very friendly with the locals.\"The researchers were surprised to find that the unicorns spoke perfect English. They were very friendly and friendly with us, and they were very friendly with the locals.The researchers were surprised to find that the unicorns spoke perfect English. They were very friendly and friendly with us, and they were very friendly with the locals.The researchers were surprised to find that the unicorns spoke perfect English. They were very friendly and friendly with us, and they\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: A train carriage containing controlled nuclear materials was stolen in Cincinnati today. Its whereabouts are unknown.\n",
      "OUTPUT: The train was carrying a shipment of nuclear materials from the United States to Japan.The train was stopped at a stoplight in the area of the Ohio River and the driver was arrested.The train was carrying a shipment of nuclear materials from the United States to Japan.The train was stopped at a stoplight in the area of the Ohio River and the driver was arrested.The train was carrying a shipment of nuclear materials from the United States to Japan.The train was stopped at a stoplight in the area of the Ohio River and the driver was arrested.The train was carrying a shipment of nuclear materials from the United States to Japan.The train was stopped at a stoplight in the area of the Ohio River and the driver was arrested.The train was carrying a shipment of nuclear materials from the United States to Japan.The train was stopped at a stoplight in the area of the Ohio River and\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.\n",
      "OUTPUT: The 21-year-old was spotted wearing a black T-shirt and black pants, and was seen walking down the street with a bag.She was later spotted walking back to her car, and was seen walking back to her car with a bag.Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard todayThe 21-year-old was spotted wearing a black T-shirt and black pants, and was seen walking down the street with a bagMiley was spotted walking back to her car, and was seen walking back to her car with a bagMiley was spotted shopping at Abercrombie and Fitch on Hollywood Boulevard todayMiley was spotted shopping at Abercrombie and Fitch on Hollywood Boulevard todayMiley was spotted shopping at Abercrombie and Fitch on Hollywood Boulevard todayMiley was spotted shopping at Abercrombie and F\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: We’ve trained a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks.\n",
      "OUTPUT: GPT-2 is a powerful tool for training and testing language models, but it's also a powerful tool for learning. We've been using it to train a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks.GPT-2 is a powerful tool for training and testing language models, but it's also a powerful tool for learning. We've been using it to train a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks.GPT-2 is a powerful tool for training and testing language models, but it's also a powerful tool\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.\n",
      "OUTPUT:  The orcs were not impressed. They charged at the hobbits, but were repelled by the hobbits' magic. The orcs were defeated, and the hobbits were left to fight alone.The hobbits were left to fight alone.The hobbits were left to fight alone.The hobbits were left to fight alone.The hobbits were left to fight alone.The hobbits were left to fight alone.The hobbits were left to fight alone.The hobbits were left to fight alone.The hobbits were left to fight alone.The hobbits were left to fight alone.The hobbits were left to fight alone.The hobbits were left to fight alone.The hobbits were left to fight alone.The hobbits were left to fight alone.The hobbits were left to fight alone.The hobbits were left\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: For today’s homework assignment, please describe the reasons for the US Civil War.\n",
      "OUTPUT: The US Civil War was a war between the Union and Confederate States of America. The war was fought from 1861 to 1865. The war was fought in the South, and the war was fought in the North. The war was fought in the South because the South was the most powerful and most powerful nation in the world. The war was fought in the North because the North was the most powerful and most powerful nation in the world. The war was fought in the South because the South was the most powerful and most powerful nation in the world. The war was fought in the North because the North was the most powerful and most powerful nation in the world. The war was fought in the South because the South was the most powerful and most powerful nation in the world. The war was fought in the North because the North was the most powerful and most powerful nation in the world. The war was fought in the South because the South was the most powerful and most powerful nation in the world. The war\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: John F. Kennedy was just elected President of the United States after rising from the grave decades after his assassination. Due to miraculous developments in nanotechnology, Kennedy’s brain was rebuilt from his remains and installed in the control center of a state-of-the art humanoid robot. Below is a transcript of his acceptance speech.\n",
      "OUTPUT: \"I am honored to accept this position as President of the United States. I am honored to be the first President of the United States to be born in the United States. I am honored to be the first President of the United States to be born in the United States. I am honored to be the first President of the United States to be born in the United States. I am honored to be the first President of the United States to be born in the United States. I am honored to be the first President of the United States to be born in the United States. I am honored to be the first President of the United States to be born in the United States. I am honored to be the first President of the United States to be born in the United States. I am honored to be the first President of the United States to be born in the United States. I am honored to be the first President of the United States to be born in the United States. I am honored\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: Recycling is good for the world.NO! YOU COULD NOT BE MORE WRONG!!\n",
      "OUTPUT: I am not a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler. I am a recycler\n",
      "\n",
      "====\n",
      "\n",
      "CPU times: user 2min 20s, sys: 34.5 s, total: 2min 54s\n",
      "Wall time: 2min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "benchmark('gpt2-medium', n_words=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 large (774M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-large with n_words=200\n",
      "=========================\n",
      "INPUT: In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "OUTPUT: The researchers, led by Dr. David R. Williams of the University of California, Santa Cruz, discovered the unicorns in the Andes Mountains of Peru. The area is known for its unique geology and is home to a number of rare species of animals.The researchers found the unicorns in the Andes Mountains of Peru.\"We were surprised to find that the unicorns were able to communicate with each other,\" Williams said. \"We were also surprised to find that they were able to communicate in English.\"The researchers believe that the unicorns are descendants of the ancient Incas, who lived in the area around 2,000 years ago.\"The Incas were the first people to use the Andes Mountains as a refuge from the harsh climate of the Andes,\" Williams said. \"They were also the first people to use the Andes Mountains as a place to hunt and gather food.\"The researchers believe that the unic\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: A train carriage containing controlled nuclear materials was stolen in Cincinnati today. Its whereabouts are unknown.\n",
      "OUTPUT: The incident occurred at about 11:30 a.m. in the area of North Main Street and East Market Street, according to the Cincinnati Police Department.The train was carrying a shipment of nuclear materials, including a fuel rod, a control rod and a control rod assembly, according to the CPD.The CPD said the incident is being investigated as a possible theft.The CPD said the incident is being investigated as a possible theft.The CPD said the incident is being investigated as a possible theft.The CPD said the incident is being investigated as a possible theft.The CPD said the incident is being investigated as a possible theft.The CPD said the incident is being investigated as a possible theft.The CPD said the incident is being investigated as a possible theft.The CPD said the incident is being investigated as a possible theft.The CPD said the\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.\n",
      "OUTPUT: The singer was spotted in the store's dressing room wearing a black top and black pants.Scroll down for videoMiley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard todayThe singer was spotted in the store's dressing room wearing a black top and black pantsThe singer was spotted in the store's dressing room wearing a black top and black pantsThe singer was spotted in the store's dressing room wearing a black top and black pantsThe singer was spotted in the store's dressing room wearing a black top and black pantsThe singer was spotted in the store's dressing room wearing a black top and black pantsThe singer was spotted in the store's dressing room wearing a black top and black pantsThe singer was spotted in the store's dressing room wearing a black top and black pantsThe singer was spotted in the store's dressing room wearing a black top and black pants\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: We’ve trained a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks.\n",
      "OUTPUT: We’ve also trained a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks. We’ve also trained a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks. We’ve also trained a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks. We’ve also trained a large language model called GPT-2 that generates realistic\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.\n",
      "OUTPUT:  The orcs were not prepared for the onslaught of the two heroes. The battle raged for hours, and the orcs were forced to retreat.The two heroes then returned to the Lonely Mountain, where they found the orcs had been defeated. The orcs had been defeated, and the two heroes were free to return to the Lonely Mountain.The two heroes then returned to the Lonely Mountain, where they found the orcs had been defeated. The orcs had been defeated, and the two heroes were free to return to the Lonely Mountain.The two heroes then returned to the Lonely Mountain, where they found the orcs had been defeated. The orcs had been defeated, and the two heroes were free to return to the Lonely Mountain.The two heroes then returned to the Lonely Mountain, where they found the orcs had been defeated. The orcs had been defeated, and the two heroes were free to return to the Lonely Mountain.The two heroes then returned to the Lonely Mountain, where\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: For today’s homework assignment, please describe the reasons for the US Civil War.\n",
      "OUTPUT: The Civil War was a war between the United States and the Confederate States of America. The war was fought between the Union and the Confederacy. The war was fought between the United States and the Confederate States of America. The war was fought between the United States and the Confederate States of America. The war was fought between the United States and the Confederate States of America. The war was fought between the United States and the Confederate States of America. The war was fought between the United States and the Confederate States of America. The war was fought between the United States and the Confederate States of America. The war was fought between the United States and the Confederate States of America. The war was fought between the United States and the Confederate States of America. The war was fought between the United States and the Confederate States of America. The war was fought between the United States and the Confederate States of America. The war was fought between the United States and the Confederate States of America. The war was fought between\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: John F. Kennedy was just elected President of the United States after rising from the grave decades after his assassination. Due to miraculous developments in nanotechnology, Kennedy’s brain was rebuilt from his remains and installed in the control center of a state-of-the art humanoid robot. Below is a transcript of his acceptance speech.\n",
      "OUTPUT: \"I am a man of my word. I will not lie to you. I will not deceive you. I will not let you down. I will not let you down. I will not let you down. I will not let you down. I will not let you down. I will not let you down. I will not let you down. I will not let you down. I will not let you down. I will not let you down. I will not let you down. I will not let you down. I will not let you down. I will not let you down. I will not let you down. I will not let you down. I will not let you down. I will not let you down. I will not let you down. I will not let you down. I will not let you down. I will not let you down. I will not let you down. I will not let you down. I will not let you down. I\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: Recycling is good for the world.NO! YOU COULD NOT BE MORE WRONG!!\n",
      "OUTPUT: The world is full of waste.The world is full of waste.The world is full of waste.The world is full of waste.The world is full of waste.The world is full of waste.The world is full of waste.The world is full of waste.The world is full of waste.The world is full of waste.The world is full of waste.The world is full of waste.The world is full of waste.The world is full of waste.The world is full of waste.The world is full of waste.The world is full of waste.The world is full of waste.The world is full of waste.The world is full of waste.The world is full of waste.The world is full of waste.\n",
      "\n",
      "====\n",
      "\n",
      "CPU times: user 4min 28s, sys: 1min 24s, total: 5min 53s\n",
      "Wall time: 5min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "benchmark('gpt2-large', n_words=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 large on custom texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-large with n_words=200\n",
      "=========================\n",
      "INPUT: World War II (often abbreviated to WWII or WW2), also known as the Second World War, was a global war that lasted from 1939 to 1945. The vast majority of the world's countries—including all the great powers—eventually formed two opposing military alliances: the Allies and the Axis. A state of total war emerged, directly involving more than 100 million people from over 30 countries. The major participants threw their entire economic, industrial, and scientific capabilities behind the war effort, blurring the distinction between civilian and military resources. World War II was the deadliest conflict in human history, marked by 70 to 85 million fatalities, most of whom were civilians in the Soviet Union and China. It included massacres, the genocide of the Holocaust, strategic bombing, premeditated death from starvation and disease, and the only use of nuclear weapons in war.[1][2][3][4]\n",
      "OUTPUT: The war was fought in Europe, Asia, and the Pacific. The Allies won the war, but the Axis lost it. The war was the first major conflict in which the United States was involved. The United States was the first country to declare war on Germany, and the first to use nuclear weapons. The United States also became the first country to use atomic weapons. The United States also became the first country to use the atomic bomb on Japan. The United States also became the first country to use the atomic bomb on Hiroshima and Nagasaki.The war was the first major conflict in which the United States was involved. The United States was the first country to declare war on Germany, and the first to use nuclear weapons. The United States also became the first country to use the atomic bomb on Japan. The United States also became the first country to use the atomic bomb on Hiroshima and Nagasaki. The war was the first major conflict in which the United States was involved. The\n",
      "\n",
      "====\n",
      "\n",
      "CPU times: user 1min 15s, sys: 23.5 s, total: 1min 38s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "ww2 = \"\"\"World War II (often abbreviated to WWII or WW2), also known as the Second World War, was a global war that lasted from 1939 to 1945. The vast majority of the world's countries—including all the great powers—eventually formed two opposing military alliances: the Allies and the Axis. A state of total war emerged, directly involving more than 100 million people from over 30 countries. The major participants threw their entire economic, industrial, and scientific capabilities behind the war effort, blurring the distinction between civilian and military resources. World War II was the deadliest conflict in human history, marked by 70 to 85 million fatalities, most of whom were civilians in the Soviet Union and China. It included massacres, the genocide of the Holocaust, strategic bombing, premeditated death from starvation and disease, and the only use of nuclear weapons in war.[1][2][3][4]\"\"\"\n",
    "benchmark('gpt2-large', n_words=200, texts=[ww2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-medium with n_words=200\n",
      "=========================\n",
      "INPUT: World War II (often abbreviated to WWII or WW2), also known as the Second World War, was a global war that lasted from 1939 to 1945. The vast majority of the world's countries—including all the great powers—eventually formed two opposing military alliances: the Allies and the Axis. A state of total war emerged, directly involving more than 100 million people from over 30 countries. The major participants threw their entire economic, industrial, and scientific capabilities behind the war effort, blurring the distinction between civilian and military resources. World War II was the deadliest conflict in human history, marked by 70 to 85 million fatalities, most of whom were civilians in the Soviet Union and China. It included massacres, the genocide of the Holocaust, strategic bombing, premeditated death from starvation and disease, and the only use of nuclear weapons in war.[1][2][3][4]\n",
      "OUTPUT: The war was fought in Europe, Asia, Africa, and the Americas. The United States, Britain, France, and Italy fought alongside Germany, Japan, and Italy. The United States, Britain, and France were the only countries to participate in the war. The United States, Britain, and France were the only countries to participate in the war.The United States, Britain, and France were the only countries to participate in the war. The United States, Britain, and France were the only countries to participate in the war. The United States, Britain, and France were the only countries to participate in the war. The United States, Britain, and France were the only countries to participate in the war. The United States, Britain, and France were the only countries to participate in the war. The United States, Britain, and France were the only countries to participate in the war. The United States, Britain, and France were the only countries to participate in the war.\n",
      "\n",
      "====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark('gpt2-medium', n_words=200, texts=[ww2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
