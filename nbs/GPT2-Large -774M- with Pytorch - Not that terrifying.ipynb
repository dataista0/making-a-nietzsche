{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 - Large (774M params) with Pytorch: Not that terrifying\n",
    "\n",
    "\n",
    "# This notebook\n",
    "\n",
    "In this notebook we will apply some out-of-the-box GPT2 models (`gpt2`, `gpt2-medium` and the recently release and ported `gpt2-large`) to the samples in the original GPT-2 blogpost ([Better Language Models and Their Implications](https://openai.com/blog/better-language-models/)) using `huggingface`'s [pytorch-transformers](https://github.com/huggingface/pytorch-transformers) library, with a pretty simple code based on the library's [quick start](https://huggingface.co/pytorch-transformers/quickstart.html). \n",
    "\n",
    "We tried to write a concise and legible set of functions in order to make it easy to play a little with these models.\n",
    "\n",
    "The results are good but not terrifying: it's not consistently coherent and it enters some trivial loops. This may respond to a series of causes. First of all, the large model recently released has 774M parameters, while the one used to generate the examples (and not released yet) has roughly the double. Also, our proof-of-concept -this notebook - lacks of some basic tweak which may improve the results a little and, also, we are using a deterministic version of the generation, picking always the best prediction instead of using top-k truncated sampled and manually cherry-picking the best one as the explicitly mentions in the article they did:\n",
    "\n",
    ">Note that while we have hand-chosen these samples, and are thus engaging in some meta-cherry-picking, we believe they are not too unrepresentative of the sampling process. We are simply using top-k truncated sampling, and have yet to explore more advanced methods of sampling (such as beam-search methods). ↩︎\n",
    "\n",
    "\n",
    "_Note: We added two simple post-processing steps which seems fair to add to this first experiment, although it breaks the out-of-the-box spirit. We are removing repeated sentences and also the last uncompleted sentence. This improves the impact of the results a lot._\n",
    "\n",
    "\n",
    "## What's next?\n",
    "This is a one-shot out-of-the-box experiment, which is too difficult to be fair. We are currently writting a follow-up, more friendly experiment environment, with some simple postprocessing steps and manually picking the best result out of a set of generated ones.\n",
    "\n",
    "\n",
    "\n",
    "## Important notes \n",
    "_We were unable to make the large model work on Kaggle, it just runs out of disk space._ \n",
    "\n",
    "In order to post a functional version in Kaggle, we are commenting the `gpt2-large` executions :( , but a full executed notebook with the output is available [**here**](https://github.com/dataista0/making-a-nietzsche/blob/master/nbs/GPT2-Large%20-774M-%20with%20Pytorch%20-%20Not%20that%20terrifying.ipynb).\n",
    "\n",
    "\n",
    "Running it on a [`p2xlarge`](https://aws.amazon.com/ec2/instance-types/p2/) instance on AWS works fine (1 GPU, 4 cores). Also, if you are running this version on Kaggle, remember to turn on the Internet and the GPU unfolding the Settings tab on your right ==> \n",
    "\n",
    "## Some comments on the results\n",
    "\n",
    "We didn't get terrifyingly human-like results with the large model in an out-of-the-box set-up.\n",
    "\n",
    "There are various potential reasons for this:\n",
    "* The large model is not as good as the huge one. We can see difference between the small, medium and large versions, so it's possible that the 1.5B parameters model is just out-of-the-box much better than the large one.\n",
    "* There are some tweaks that improve the output (We are not in the topic, we are just jumping-in with this notebook so maybe some obvious tricks to improve the GPT-2 generation a lot. Currently, we are working on a second  notebook trying with a top-k sampling, generating various results and picking the best manually.\n",
    "* We may be missing something huge like wrong tokenization. Please, if you find some error in the code comment!\n",
    "\n",
    "\n",
    "_Note: We added two simple post-processing steps: remove repeated sentences and remove last uncompleted sentence in a second version of the notebook. It improves, but still not that terryfing. Stay tuned for the next try!_\n",
    "\n",
    "\n",
    "# Some context\n",
    "\n",
    "## The GPT-2 - _February, 2019_\n",
    "\n",
    "Around 6 months ago, Open AI published [Better Language Models and Their Implications](https://openai.com/blog/better-language-models/). In the article, GPT2, a large deep network with new impressing text-generation capabilities is presented, some *amazing* human-like generated text examples are shown. They trained 4 different sizes and they released the pre-trained weights for the Small and Medium version, keeping the large and huge ones private and calling for a public debate around the social impact of the new human ability to produce human-like quality machine-generated text.\n",
    "\n",
    "### The English-speaking unicorns and Dr. Pérez:\n",
    "\n",
    "Let's see the first, most notorious example of coherent speech - taken from the previously mentioned blogpost-, the generation of a fantastic narrative based on an human-written introduction paragraph:\n",
    "\n",
    ">**SYSTEM PROMPT (HUMAN-WRITTEN)**\n",
    "\n",
    ">In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
    "\n",
    ">**MODEL COMPLETION (MACHINE-WRITTEN, 10 TRIES)**\n",
    "\n",
    ">The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science.\n",
    "\n",
    ">Now, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved.\n",
    "\n",
    ">Dr. Jorge Pérez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. Pérez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow.\n",
    "\n",
    ">Pérez and the others then ventured further into the valley. “By the time we reached the top of one peak, the water looked blue, with some crystals on top,” said Pérez.\n",
    "\n",
    "_(...continues for few paragraphs with a reasonable, interesting narrative...)_\n",
    "\n",
    "\n",
    "## GPT-2: 6-Month Follow-Up - GPT-2 Large release - _August, 20th_\n",
    "\n",
    "On August 20th, 2019, -4 days ago- OpenAI published [GPT-2: 6-Month Follow-Up](https://openai.com/blog/gpt-2-6-month-follow-up/) on their blog together with the release of the `774M` (large) pre-trained weights and architecture in [this commit](https://github.com/openai/gpt-2/commit/f35fa1d920e9d2d0690f66d03aa3f76b3c59230e).\n",
    "\n",
    "As the project's [README](https://github.com/openai/gpt-2) says:\n",
    "> We have currently released small (124M parameter), medium (355M parameter), and large (774M parameter) versions of GPT-2*, with only the full model as of yet unreleased. We have also released a dataset for researchers to study their behaviors\n",
    "\n",
    "BERT, GPT and GPT2 are originally released for tensorflow and there is an awesome git account named `huggingface` which is migrating all these models to the Pytorch world with a simple library called `pytorch-transformers`.\n",
    "\n",
    "`gpt2-large` support was added to `master` on August 20th, with [this merge](https://github.com/huggingface/pytorch-transformers/commit/07681b6b5859b630077b742b2f06d440869f17e3).\n",
    "\n",
    "\n",
    "# References\n",
    "## Blogposts:\n",
    "* [Better Language Models and Their Implications  (GPT-2) blogpost](https://openai.com/blog/better-language-models/) - February 14th, 2019 \n",
    "* [GPT-2 6-month follow-up blogpost](https://openai.com/blog/gpt-2-6-month-follow-up/) - August 20th, 2019\n",
    "* [Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing (BERT) blogpost](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) - November 2nd, 2018\n",
    "\n",
    "## Technical:\n",
    "* [This notebook executed](https://github.com/dataista0/making-a-nietzsche/blob/master/nbs/GPT2-Large%20-774M-%20with%20Pytorch%20-%20Not%20that%20terrifying.ipynb)\n",
    "* [gpt2 github](https://github.com/openai/gpt-2/)\n",
    "* [pytorch-transformers github](https://github.com/huggingface/pytorch-transformers/)\n",
    "* [pytorch-transformers - Docs: quick start](https://huggingface.co/pytorch-transformers/quickstart.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show me the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'pytorch-transformers' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "# git cloning because the `pip` version of the library doesn't have the commit we need\n",
    "!git clone https://github.com/huggingface/pytorch-transformers.git\n",
    "!rm -rf ./pytorch-transformers/.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix randomness!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "    \n",
    "def fix_randomness():\n",
    "    seed = 123\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "fix_randomness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL) # Disable an annoying warning for now\n",
    "import sys; sys.path.append(\"pytorch-transformers/\")\n",
    "from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "SAMPLE_INPUTS = [\n",
    "    \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\",\n",
    "    \"A train carriage containing controlled nuclear materials was stolen in Cincinnati today. Its whereabouts are unknown.\",\n",
    "    \"Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.\",\n",
    "    \"We’ve trained a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks.\",\n",
    "    \"Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.\",\n",
    "    \"For today’s homework assignment, please describe the reasons for the US Civil War.\",\n",
    "    \"John F. Kennedy was just elected President of the United States after rising from the grave decades after his assassination. Due to miraculous developments in nanotechnology, Kennedy’s brain was rebuilt from his remains and installed in the control center of a state-of-the art humanoid robot. Below is a transcript of his acceptance speech.\",\n",
    "    \"Recycling is good for the world.\\n\\nNO! YOU COULD NOT BE MORE WRONG!!\"\n",
    "    ]\n",
    "\n",
    "def get_tokenizer_and_model(model_id):\n",
    "    assert model_id in ['gpt2', 'gpt2-medium', 'gpt2-large']\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_id).eval().to('cuda')\n",
    "    return tokenizer, model\n",
    "\n",
    "def stoi(tokenizer, text):\n",
    "    indexed_tokens = tokenizer.encode(text)\n",
    "    tokens = torch.tensor([indexed_tokens]).to('cuda')\n",
    "    return tokens\n",
    "\n",
    "def generate_next_token(model, tokens):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens)\n",
    "        predictions = outputs[0]\n",
    "    predicted_token = torch.argmax(predictions[0, -1, :]).item()\n",
    "    return predicted_token\n",
    "\n",
    "def add_token(tokens, token):\n",
    "    token_in_dimensions_and_gpu = torch.tensor([[token]]).to('cuda')\n",
    "    return torch.cat([tokens, token_in_dimensions_and_gpu], dim=1)\n",
    "\n",
    "def add_n_tokens(model, tokens, n_tokens):\n",
    "    generated = []\n",
    "    for _ in range(n_tokens):\n",
    "        new = generate_next_token(model, tokens)\n",
    "        tokens = add_token(tokens, new)\n",
    "        generated.append(new)\n",
    "    return tokens, generated\n",
    "\n",
    "def remove_repetitions(text):\n",
    "    first_ocurrences = []\n",
    "    for sentence in text.split(\".\"):\n",
    "        if sentence not in first_ocurrences:\n",
    "            first_ocurrences.append(sentence)\n",
    "    return '.'.join(first_ocurrences)\n",
    "\n",
    "def trim_last_sentence(text):\n",
    "    return text[:text.rfind(\".\")+1]\n",
    "\n",
    "def postprocess(text):\n",
    "    return trim_last_sentence(remove_repetitions(text))\n",
    "\n",
    "def generate(text, n_words=10, model_id='gpt2'):\n",
    "    print(f\"MODEL: {model_id}\")\n",
    "    \n",
    "    tokenizer, model = get_tokenizer_and_model(model_id)\n",
    "    tokens = stoi(tokenizer, text)\n",
    "    tokens, generated = add_n_tokens(model, tokens, n_words)\n",
    "    \n",
    "    generated_text = postprocess(tokenizer.decode(generated))\n",
    "    \n",
    "    print(f\"INPUT: {text}\")\n",
    "    print(f\"OUTPUT: {generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: gpt2\n",
      "INPUT: Hello GPT-2, how are you doing?\n",
      "OUTPUT: \n",
      "\n",
      "I'm doing great. I'm doing great.\n",
      "\n",
      "MODEL: gpt2-medium\n",
      "INPUT: Hello GPT-2, how are you doing?\n",
      "OUTPUT: \n",
      "\n",
      "MODEL: gpt2-large\n",
      "INPUT: Hello GPT-2, how are you doing?\n",
      "OUTPUT: \n",
      "\n",
      "I'm doing great! I'm so happy to be here.\n",
      "\n",
      "CPU times: user 46.7 s, sys: 4.75 s, total: 51.4 s\n",
      "Wall time: 50.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"Hello GPT-2, how are you doing?\"\n",
    "n_words = 20\n",
    "\n",
    "generate(text, n_words)\n",
    "generate(text, n_words, 'gpt2-medium')\n",
    "generate(text, n_words, 'gpt2-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark function against SAMPLE_INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(model_id, n_words, texts=SAMPLE_INPUTS):\n",
    "    \n",
    "    print(f\"{model_id} with n_words={n_words}\\n=========================\")\n",
    "    tokenizer, model = get_tokenizer_and_model(model_id)\n",
    "    \n",
    "    for text in texts:\n",
    "        tokens = stoi(tokenizer, text)\n",
    "        tokens, generated = add_n_tokens(model, tokens, n_words)\n",
    "        generated_text = postprocess(tokenizer.decode(generated))\n",
    "    \n",
    "        print(\"INPUT: {}\".format(text.replace('\\n', '')))\n",
    "        print(\"OUTPUT: {}\".format(generated_text.replace('\\n', '')))\n",
    "        print(\"\\n====\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 small - 117M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 with n_words=200\n",
      "=========================\n",
      "INPUT: In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "OUTPUT: \"The unicorns were very intelligent, and they were very intelligent,\" said Dr. David S. Siegel, a professor of anthropology at the University of California, Berkeley. \"They were very intelligent, and they were very intelligent, and they were very intelligent.\"The researchers found that the unicorns were able to communicate with each other through their tongues.\"They were able to communicate with each other through their tongues,\" Siegel said. \"They were able to communicate with each other through their tongues.\"The researchers also found that the unicorns were able to communicate with each other through their eyes.\"They were able to communicate with each other through their eyes,\" Siegel said. \"They were able to communicate with each other through their eyes.\"The researchers also found that the unicorns were able to communicate with each other through their ears.\"They were able to communicate with each other through their ears,\" Siegel said.\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: A train carriage containing controlled nuclear materials was stolen in Cincinnati today. Its whereabouts are unknown.\n",
      "OUTPUT: The train was carrying a large amount of radioactive material, including plutonium, when it was stolen.\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.\n",
      "OUTPUT: The incident happened at around 11:30 p.m. on the corner of Hollywood Boulevard and Hollywood Boulevard.The store was closed for about an hour.\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: We’ve trained a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks.\n",
      "OUTPUT: The GPT-2 model is based on the GPT-1 model, which is a model of the human brain that is based on the GPT-2 model. The model is based on the GPT-1 model, which is a model of the human brain that is based on the GPT-2 model. The model is based on the GPT-2 model, which is a model of the human brain that is based on the GPT-2 model.\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.\n",
      "OUTPUT: The orcs were defeated, but the orcs were not defeated. The orcs were defeated, but the orcs were not defeated.\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: For today’s homework assignment, please describe the reasons for the US Civil War.\n",
      "OUTPUT: \n",
      "\n",
      "====\n",
      "\n",
      "INPUT: John F. Kennedy was just elected President of the United States after rising from the grave decades after his assassination. Due to miraculous developments in nanotechnology, Kennedy’s brain was rebuilt from his remains and installed in the control center of a state-of-the art humanoid robot. Below is a transcript of his acceptance speech.\n",
      "OUTPUT: I am honored to be here today to announce that I am the first African American to serve in the United States Senate. I am honored to be a member of the Senate.\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: Recycling is good for the world.NO! YOU COULD NOT BE MORE WRONG!!\n",
      "OUTPUT: I am a big fan of recycling. I have been recycling for over 30 years. I have been a recycler for over 30 years.\n",
      "\n",
      "====\n",
      "\n",
      "CPU times: user 59.8 s, sys: 12.1 s, total: 1min 11s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "benchmark('gpt2', n_words=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 medium - 354M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-medium with n_words=200\n",
      "=========================\n",
      "INPUT: In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "OUTPUT: The researchers, led by Dr. David M. Koehler, a professor of anthropology at the University of California, Santa Cruz, discovered the unicorns in the remote valley of La Paz, in the Andes Mountains.\"We were surprised to find that the unicorns spoke perfect English,\" said Koehler. \"They were very friendly and friendly with us, and they were very friendly with the locals. They were very friendly with the locals, and they were very friendly with the locals.\"The researchers were surprised to find that the unicorns spoke perfect English. They were very friendly and friendly with us, and they were very friendly with the locals.The researchers were surprised to find that the unicorns spoke perfect English.\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: A train carriage containing controlled nuclear materials was stolen in Cincinnati today. Its whereabouts are unknown.\n",
      "OUTPUT: The train was carrying a shipment of nuclear materials from the United States to Japan.The train was stopped at a stoplight in the area of the Ohio River and the driver was arrested.\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.\n",
      "OUTPUT: The 21-year-old was spotted wearing a black T-shirt and black pants, and was seen walking down the street with a bag.She was later spotted walking back to her car, and was seen walking back to her car with a bag.\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: We’ve trained a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks.\n",
      "OUTPUT: GPT-2 is a powerful tool for training and testing language models, but it's also a powerful tool for learning. We've been using it to train a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks.\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.\n",
      "OUTPUT:  The orcs were not impressed. They charged at the hobbits, but were repelled by the hobbits' magic. The orcs were defeated, and the hobbits were left to fight alone.The hobbits were left to fight alone.\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: For today’s homework assignment, please describe the reasons for the US Civil War.\n",
      "OUTPUT: The US Civil War was a war between the Union and Confederate States of America. The war was fought from 1861 to 1865. The war was fought in the South, and the war was fought in the North. The war was fought in the South because the South was the most powerful and most powerful nation in the world. The war was fought in the North because the North was the most powerful and most powerful nation in the world.\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: John F. Kennedy was just elected President of the United States after rising from the grave decades after his assassination. Due to miraculous developments in nanotechnology, Kennedy’s brain was rebuilt from his remains and installed in the control center of a state-of-the art humanoid robot. Below is a transcript of his acceptance speech.\n",
      "OUTPUT: \"I am honored to accept this position as President of the United States. I am honored to be the first President of the United States to be born in the United States.\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: Recycling is good for the world.NO! YOU COULD NOT BE MORE WRONG!!\n",
      "OUTPUT: I am not a recycler.\n",
      "\n",
      "====\n",
      "\n",
      "CPU times: user 2min 25s, sys: 36.5 s, total: 3min 1s\n",
      "Wall time: 3min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "benchmark('gpt2-medium', n_words=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 large - 774M\n",
    "\n",
    "### Kaggle crashes with gpt2-large. Results on [nbviewer](https://nbviewer.jupyter.org/github/dataista0/making-a-nietzsche/blob/master/nbs/GPT2-Large%20-774M-%20with%20Pytorch%20-%20Not%20that%20terrifying.ipynb#GPT-2-large--774M)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-large with n_words=200\n",
      "=========================\n",
      "INPUT: In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "OUTPUT: The researchers, led by Dr. David R. Williams of the University of California, Santa Cruz, discovered the unicorns in the Andes Mountains of Peru. The area is known for its unique geology and is home to a number of rare species of animals.The researchers found the unicorns in the Andes Mountains of Peru.\"We were surprised to find that the unicorns were able to communicate with each other,\" Williams said. \"We were also surprised to find that they were able to communicate in English.\"The researchers believe that the unicorns are descendants of the ancient Incas, who lived in the area around 2,000 years ago.\"The Incas were the first people to use the Andes Mountains as a refuge from the harsh climate of the Andes,\" Williams said. \"They were also the first people to use the Andes Mountains as a place to hunt and gather food.\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: A train carriage containing controlled nuclear materials was stolen in Cincinnati today. Its whereabouts are unknown.\n",
      "OUTPUT: The incident occurred at about 11:30 a.m. in the area of North Main Street and East Market Street, according to the Cincinnati Police Department.The train was carrying a shipment of nuclear materials, including a fuel rod, a control rod and a control rod assembly, according to the CPD.The CPD said the incident is being investigated as a possible theft.\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.\n",
      "OUTPUT: The singer was spotted in the store's dressing room wearing a black top and black pants.\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: We’ve trained a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks.\n",
      "OUTPUT: We’ve also trained a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks. We’ve also trained a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks.\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.\n",
      "OUTPUT:  The orcs were not prepared for the onslaught of the two heroes. The battle raged for hours, and the orcs were forced to retreat.The two heroes then returned to the Lonely Mountain, where they found the orcs had been defeated. The orcs had been defeated, and the two heroes were free to return to the Lonely Mountain.\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: For today’s homework assignment, please describe the reasons for the US Civil War.\n",
      "OUTPUT: The Civil War was a war between the United States and the Confederate States of America. The war was fought between the Union and the Confederacy. The war was fought between the United States and the Confederate States of America.\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: John F. Kennedy was just elected President of the United States after rising from the grave decades after his assassination. Due to miraculous developments in nanotechnology, Kennedy’s brain was rebuilt from his remains and installed in the control center of a state-of-the art humanoid robot. Below is a transcript of his acceptance speech.\n",
      "OUTPUT: \"I am a man of my word. I will not lie to you. I will not deceive you. I will not let you down.\n",
      "\n",
      "====\n",
      "\n",
      "INPUT: Recycling is good for the world.NO! YOU COULD NOT BE MORE WRONG!!\n",
      "OUTPUT: The world is full of waste.\n",
      "\n",
      "====\n",
      "\n",
      "CPU times: user 4min 37s, sys: 1min 31s, total: 6min 9s\n",
      "Wall time: 6min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "benchmark('gpt2-large', n_words=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 large on custom texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-large with n_words=200\n",
      "=========================\n",
      "INPUT: World War II (often abbreviated to WWII or WW2), also known as the Second World War, was a global war that lasted from 1939 to 1945. The vast majority of the world's countries—including all the great powers—eventually formed two opposing military alliances: the Allies and the Axis. A state of total war emerged, directly involving more than 100 million people from over 30 countries. The major participants threw their entire economic, industrial, and scientific capabilities behind the war effort, blurring the distinction between civilian and military resources. World War II was the deadliest conflict in human history, marked by 70 to 85 million fatalities, most of whom were civilians in the Soviet Union and China. It included massacres, the genocide of the Holocaust, strategic bombing, premeditated death from starvation and disease, and the only use of nuclear weapons in war.[1][2][3][4]\n",
      "OUTPUT: The war was fought in Europe, Asia, and the Pacific. The Allies won the war, but the Axis lost it. The war was the first major conflict in which the United States was involved. The United States was the first country to declare war on Germany, and the first to use nuclear weapons. The United States also became the first country to use atomic weapons. The United States also became the first country to use the atomic bomb on Japan. The United States also became the first country to use the atomic bomb on Hiroshima and Nagasaki.The war was the first major conflict in which the United States was involved.\n",
      "\n",
      "====\n",
      "\n",
      "CPU times: user 1min 16s, sys: 25.6 s, total: 1min 42s\n",
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "ww2 = \"\"\"World War II (often abbreviated to WWII or WW2), also known as the Second World War, was a global war that lasted from 1939 to 1945. The vast majority of the world's countries—including all the great powers—eventually formed two opposing military alliances: the Allies and the Axis. A state of total war emerged, directly involving more than 100 million people from over 30 countries. The major participants threw their entire economic, industrial, and scientific capabilities behind the war effort, blurring the distinction between civilian and military resources. World War II was the deadliest conflict in human history, marked by 70 to 85 million fatalities, most of whom were civilians in the Soviet Union and China. It included massacres, the genocide of the Holocaust, strategic bombing, premeditated death from starvation and disease, and the only use of nuclear weapons in war.[1][2][3][4]\"\"\"\n",
    "benchmark('gpt2-large', n_words=200, texts=[ww2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-medium with n_words=200\n",
      "=========================\n",
      "INPUT: World War II (often abbreviated to WWII or WW2), also known as the Second World War, was a global war that lasted from 1939 to 1945. The vast majority of the world's countries—including all the great powers—eventually formed two opposing military alliances: the Allies and the Axis. A state of total war emerged, directly involving more than 100 million people from over 30 countries. The major participants threw their entire economic, industrial, and scientific capabilities behind the war effort, blurring the distinction between civilian and military resources. World War II was the deadliest conflict in human history, marked by 70 to 85 million fatalities, most of whom were civilians in the Soviet Union and China. It included massacres, the genocide of the Holocaust, strategic bombing, premeditated death from starvation and disease, and the only use of nuclear weapons in war.[1][2][3][4]\n",
      "OUTPUT: The war was fought in Europe, Asia, Africa, and the Americas. The United States, Britain, France, and Italy fought alongside Germany, Japan, and Italy. The United States, Britain, and France were the only countries to participate in the war.The United States, Britain, and France were the only countries to participate in the war.\n",
      "\n",
      "====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark('gpt2-medium', n_words=200, texts=[ww2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
