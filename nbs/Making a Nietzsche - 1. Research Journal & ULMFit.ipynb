{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Nietzsche - Day 1. Research Journal and simple out-of-the-box ULMFit\n",
    "\n",
    "## Welcome to my transfer learning for nlp research journal\n",
    "\n",
    "Long story short, trying to **get acquainted with the most recent nlp pre-trained models (ULMFit, BERT, GPT, GPT-2 and XLNet)** I'm trying to create a language model able to **generate Nietzsche-like prose of a reasonable quality**. I just started and **my results are not even close to good...** but I think the process of trying to create one using these technologies is interesting by itself, so I'm writing **a research journal** and not a result-oriented post.\n",
    "\n",
    "For now, these are the plans:\n",
    "\n",
    "1. [Day 1 - Making a Nietzsche - Research Journal and simple out-of-the-box ULMFit](https://github.com/dataista0/making-a-nietzsche/blob/master/nbs/Making%20a%20Nietzsche%20-%201.%20Research%20Journal%20%26%20ULMFit.ipynb) (this notebook)\n",
    "2. [Day 2 - Out-of-the-box GPT-2 Large](https://www.kaggle.com/julian3833/gpt2-large-774m-w-pytorch-not-that-impressing) (new!)\n",
    "3. *Making a Nietzsche - Day 3 - GPT-2 for Nietzsche* (Soon!)\n",
    "\n",
    "\n",
    "4. Define what to do from here... (train more, find better data, another architecture, another thing)\n",
    "\n",
    "\n",
    "### The last self-love wound to humanity\n",
    "\n",
    "Just a brief non-technical personal opinion considering the philosophical impact of the ability to train machines to generate human-like prose. You can skip it if it's not your thing.\n",
    "\n",
    "\n",
    "In *Introduction to Psychoanalysis*, Freud proposes the notion of three big narcissistic wounds of humanity in its self-perception through science and theory: \n",
    "\n",
    "> Humanity has in the course of time had to endure from the hands of science two great outrages upon its naive self-love. \n",
    "The first was when it realized that our earth was not the center of the universe, but only a tiny speck in a \n",
    "world-system of a magnitude hardly conceivable; this is associated in our minds with the name of Copernicus, ...\n",
    "The second was when biological research robbed man of his peculiar privilege of having been specially created, and \n",
    "relegated him to a descent from the animal world, implying an ineradicable animal nature in him: this transvaluation \n",
    "has been accomplished in our own time upon the instigation of Charles Darwin... \n",
    "But man's craving for grandiosity is now suffering the third and most bitter blow from present-day psychological \n",
    "research which is endeavoring to prove to the ego of each one of us that he is not even master in his own house, but \n",
    "that he must remain content with the veriest scraps of information about what is going on unconsciously in his own mind.\n",
    "    \n",
    "I consider the ability of machines to generate human-like prose like a narcissistic wound in the same sense: it robbes us something we considered esencial to our nature, and a marked differential skill which made us completely different and more important that the rest of the existence: the ability to produce language, a coherent speech. \n",
    "\n",
    "Since the deep learning-for-nlp boom during late 2018 I have the intention to get familiar with transfer learning in NLP. This is a personal project in which I will try to take to life all the theory and novelties that are in the air trying to reproduce Nietzsche prose with state-of-art generative models. \n",
    "\n",
    "\n",
    "### Message for forkers: remember to turn on the GPU and Internet!\n",
    "\n",
    "# Day 1: shitty data and out-of-the-box ULMFit with default parameters\n",
    "\n",
    "Today we are just jumping-in. We will setup the full workflow for a simple trivial model. We will iterate after.\n",
    "\n",
    "I'm currently learning fast.ai, so I will start using ULMFit wikitext 103 pre-trained weights, which are way less powerfull, I think, than GPT-2. I'm also not sure if BERT can generate text, but I know for sure that OpenAI's GPT-2 [can](https://openai.com/blog/better-language-models/) and I'm willing to get my hands on!\n",
    "\n",
    "## ULMFit out-of-the-box without fine-tuning\n",
    "\n",
    "Turns out that the unsupervised task Jeremy used to create ULMFit is vanilla language modelling, which means the network without any finetuning knows how to generate text.\n",
    "But let's check-it-out.\n",
    "\n",
    "In a nutshell, fast.ai is a super high-level deep learning API. It is not the equivalent of Keras but the equivalent of something that build on top of Keras are gives structure and automates some high-level workflows which were not automated before.\n",
    "The main concept is the Learner. \n",
    "\n",
    "A `Learner` trains a `model` on some `train data`, validating against some `validation data` using an `optimizer`, a `loss` function and zero or more `metrics`.\n",
    "So the signature of a learner is something like: `Learner(data, model, optimizer, loss, metrics)`.\n",
    "\n",
    "Each of the four supported deep learning applications (`text`, `vision`, `tabular`, `collab` and `vision.gan`) have a high-level function for creating a good out-of-the-box learner with some good pre-trained weights specifying only the data. \n",
    "\n",
    "The data is modelled as a `Databunch`, which is just set containing a train, a validation and optionaly a test datasets, aka: 2 or 3 lists of (x, y) pairs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on reproducibility\n",
    "\n",
    "We are using a lot of random functions and method, so we need to fix the randomness in order to achieve reproducibility.\n",
    "We need to set numpy's random seed but also two torch ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reproducible():\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    seed = 123\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "make_reproducible()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a databunch from a list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextLMDataBunch;\n",
       "\n",
       "Train: LabelList (975 items)\n",
       "x: LMTextList\n",
       "xxbos fake sentence 1 .,xxbos fake sentence 2 .,xxbos fake sentence 1 .,xxbos fake sentence 2 .,xxbos fake sentence 1 .\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (25 items)\n",
       "x: LMTextList\n",
       "xxbos fake sentence 2 .,xxbos fake sentence 1 .,xxbos fake sentence 2 .,xxbos fake sentence 1 .,xxbos fake sentence 2 .\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fastai.text import TextLMDataBunch, language_model_learner, AWD_LSTM\n",
    "\n",
    "# Amount of sentences to use as validation. This is hardcoded for simplicity.\n",
    "N_VAL = 25\n",
    "\n",
    "def get_lm_databunch(sentences):\n",
    "    \"\"\" Create a `TextLMDataBunch` for list of sentences \"\"\"\n",
    "    train_df = pd.DataFrame({'label': 0, 'text': sentences[:-N_VAL]}) # This is how you specify data for language modeling\n",
    "    valid_df = pd.DataFrame({'label': 0, 'text': sentences[-N_VAL:]})    \n",
    "    return TextLMDataBunch.from_df(path='.', train_df=train_df, valid_df=valid_df, bs=192)\n",
    "\n",
    "\n",
    "data_lm = get_lm_databunch([\"fake sentence 1.\", \"fake sentence 2.\"]*500)\n",
    "data_lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a pre-trained ULMFit language model for a given databunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The vocabulary is tied to the databunch, which had only 16 character\n",
    "len(learn.data.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xxunk', 'xxpad', 'xxbos', 'xxeos', 'xxfld', 'xxmaj', 'xxup', 'xxrep', 'xxwrep', 'fake', 'sentence', '.', '1', '2', 'xxfake', 'xxfake']\n"
     ]
    }
   ],
   "source": [
    "print(learn.data.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake sentence 1. fake sentence Sentence 2 SENTENCE Sentence Sentence Sentence Sentence . Sentence Sentence .\n"
     ]
    }
   ],
   "source": [
    "# It predicts \n",
    "sample = \"\"\"fake sentence 1. fake\"\"\"\n",
    "print(learn.predict(sample, n_words=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:31 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.265474</td>\n",
       "      <td>6.054400</td>\n",
       "      <td>0.185714</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.066381</td>\n",
       "      <td>5.760055</td>\n",
       "      <td>0.185714</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.002691</td>\n",
       "      <td>5.151420</td>\n",
       "      <td>0.185714</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.899597</td>\n",
       "      <td>4.478835</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.733174</td>\n",
       "      <td>3.911566</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.511778</td>\n",
       "      <td>3.435199</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5.229397</td>\n",
       "      <td>3.053329</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.992468</td>\n",
       "      <td>2.794451</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.803389</td>\n",
       "      <td>2.656074</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.613796</td>\n",
       "      <td>2.615304</td>\n",
       "      <td>0.385714</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(10, max_lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake sentence 1. fake sentence 2 . Sentence 2 . Sentence 2 . Fake sentence . . . 1 .\n"
     ]
    }
   ],
   "source": [
    "# It improves a little\n",
    "print(learn.predict(\"fake sentence 1. fake\", n_words=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's find out what Zarathustra has to say\n",
    "\n",
    "Here we get some sentences from the Zarathustra. The exploration about cleaning was collapsed into a minimal legible function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3319"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import bs4\n",
    "import numpy as np\n",
    "\n",
    "ZARATHUSTRA = \"https://archive.org/stream/thusspokezarathu00nietuoft/thusspokezarathu00nietuoft_djvu.txt\"\n",
    "\n",
    "def get_sentences():\n",
    "    response = requests.get(ZARATHUSTRA)\n",
    "    html = response.content\n",
    "    soup = bs4.BeautifulSoup(html, 'html.parser')\n",
    "    txt_data = soup.pre.contents[0]\n",
    "    pattern = re.compile(r\"\\n\\n\\n[ \\w'\\(\\)]+\\n\\n\", re.MULTILINE)\n",
    "    txt_data = re.sub(pattern, '', txt_data)\n",
    "    lines = txt_data.split(\"\\n\")\n",
    "    \n",
    "    lines = [l for l in lines if len(l) > 1 and not l.isupper() and not l.istitle()]\n",
    "    \n",
    "    # Collapse the \\n (they are breaking sentences in a wide format), \n",
    "    # After collapsing the \\n we got a large string, split it by \".\"\n",
    "    # Add again the . at the end of the lines\n",
    "    sentences = [ f\"{l}.\" for l in (' '.join(lines)).split(\".\")]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "# We bring 3319 sentences from Zarathustra\n",
    "s = get_sentences()\n",
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"  It is a lie! Creators were they who created peoples, and hung  a faith and a love over them : thus they served life.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking everything looks good\n",
    "np.random.choice(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 04:37 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.452039</td>\n",
       "      <td>3.121433</td>\n",
       "      <td>0.410417</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.266574</td>\n",
       "      <td>3.085846</td>\n",
       "      <td>0.372247</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.998517</td>\n",
       "      <td>3.004556</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.737266</td>\n",
       "      <td>2.712997</td>\n",
       "      <td>0.445164</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.509505</td>\n",
       "      <td>2.617201</td>\n",
       "      <td>0.462946</td>\n",
       "      <td>00:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.317868</td>\n",
       "      <td>2.534178</td>\n",
       "      <td>0.484003</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.157368</td>\n",
       "      <td>2.518567</td>\n",
       "      <td>0.489509</td>\n",
       "      <td>00:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.025093</td>\n",
       "      <td>2.502783</td>\n",
       "      <td>0.494568</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.920690</td>\n",
       "      <td>2.492634</td>\n",
       "      <td>0.497693</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.844982</td>\n",
       "      <td>2.507464</td>\n",
       "      <td>0.494345</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_trained_lm(sentences):\n",
    "    data_lm = get_lm_databunch(sentences)\n",
    "    learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5)\n",
    "    learn.fit_one_cycle(10, 1e-2)\n",
    "    return learn\n",
    "\n",
    "lm = get_trained_lm(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence:   Then lived they shamelessly in temporary pleasures, and  beyond the day had hardly an aim.\n",
      "\n",
      "\n",
      "Generation:   Then lived they shamelessly in temporary pleasures, and  beyond the day had hardly an aim. At last they held the trembling hands of one another and lived hours full of noise ; then should i bear them to avoid sleeping and not to bear be the spirit of a man . xxbos For everything that is around it , it is time for future or fragrance to meet with an enemy : for breast - through - eyes often sit beside your hearts . xxbos my reason is that God may be treated on this patient patient , sneezeth it , when at night such circumstances have come \" : they be quite nondescript ; and they make a hard conscience what came before me . So it is the great fate to strength there . \" This thing is more of a story than i have lieth in public opinion . xxbos But to not be able to be sure , i am very weary of choosing Zarathustra to torturest his heart . xxbos But there is also him scorn for weeping in teaching . He hath even policy of himself to one sweat : the sign to the heart is to sit for himself\n"
     ]
    }
   ],
   "source": [
    "def show_sample(lm, s):\n",
    "    ss = np.random.choice(s)\n",
    "    print(f\"Input sentence: {ss}\\n\\n\")\n",
    "    print(f\"Generation: {lm.predict(ss, n_words=200)}\")\n",
    "show_sample(lm, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4616"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This thing produces this amount of words I think.\n",
    "len(lm.data.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence:   \"From on high,\" drippeth the star, and the gracious spittle;  for the hi^h, longeth every starless bobom.\n",
      "\n",
      "\n",
      "Generation:   \"From on high,\" drippeth the star, and the gracious spittle;  for the hi^h, longeth every starless bobom. xxbos \" already our old hearts have now turned their heads on earth ! Already the Super ones ! Now did we make this mantle new happiness ! Have they ever this flock ? How could your fruits become smaller , according to your values ! xxbos And even with a valuing . xxbos Do ye divine only ones ! By your submission : i call your hands , secrets , and knowledge ? xxbos And had three soldiers be killed in this war ? And there is not much in the world ! Anne is the day , and the world was about to be revealed ; it had begun to press up . xxbos Beyond the Star and 51 - 30 - 235 , the latter is saying everything round he may also be a failure . \" a good bite about the serpent Santa Margherita came to me , and but that did he always bite ; and now learn of my conjecturing , that i have again changed . xxbos Now do i let his animals live God s\n"
     ]
    }
   ],
   "source": [
    "show_sample(lm, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'God is a non-sense, a spiritual weakness of a degraded society. They possess an outward and type so far as they could . xxbos There is much tension between God and the man ; in the moonlight and the path from the gods , it is persistent that God maketh the lightning in the depth . The other feminine form of something which lay home to one another is the Superman , formerly the Superman . a middle - born man , calling similar who jewel his whip with the past , is called the Purple Superman . However , the true meaning of the truth can nor express the coming of the new spirit , The Dead . The \" Superman \" already called itself the Superman is only an attempt . xxbos Such a star also seem to ye being older than today , but to create that it is perfect because it is only a little harder than one else * all things . xxbos Once i began to see in the last one no longer ingly , and all lands are my least , whilst ye did my own ruined'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not that bad, huh?\n",
    "lm.predict(\"God is a non-sense, a spiritual weakness of a degraded society.\", n_words=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When Zarathustra was thirty years old, he left his home and the lake of his home, and went into the mountains. There he \n",
      "enjoyed his spirit and his solitude, and for ten years did not weary of it. But at last his heart changed, and rising one \n",
      "morning with the rosy dawn, he went before the sun, and spake thus unto it: \"Thou great star! What would be thy happiness if thou hadst not those for whom thou shinest! \n",
      "For ten years hast thou climbed hither unto my cave: thou wouldsthave wearied of thy light and of the journey, had it not been for me, mine eagle, and my serpent. \n",
      "But we awaited thee every morning, took from thee thine overflow, and blessed thee for it\".  \n",
      " All of his joyous aspiration is to the sake of the evil and the virtues All things, all the creating, the holy will of all things ; and by earth it is to be all loving and loved : full of force to be surpassed : truth would every one, with all pharisees feel the swine and values of power, they know thou also must have an \" impatient \" one \n",
      " For deaf ones ;, even if there is meant unto it, the required ears are not of deep perception ; all animals are selfishness - weary lonesomest ones, \" i call the manner of going, and the stream of them ! High men lip coiled around their feet namely how most bad then is \" \" Alas, how ever could not get rid of the wan abyss ? And get out the clock - bell ! The great camel did come with the camel Thus was the devil who wanted to put the golden ball into its mouth \" The murderer Streams and\n"
     ]
    }
   ],
   "source": [
    "# The extra spaces are annoying me\n",
    "def predict(lm, sample, n_words=200):\n",
    "    p = lm.predict(sample, n_words=n_words).replace(\" ,\", \",\").replace(\" .\", \"\").replace(\"xxbos\", \"\\n\")\n",
    "    print(p)\n",
    "    return p\n",
    "\n",
    "THE_BEGGINING=\"\"\"When Zarathustra was thirty years old, he left his home and the lake of his home, and went into the mountains. There he \n",
    "enjoyed his spirit and his solitude, and for ten years did not weary of it. But at last his heart changed, and rising one \n",
    "morning with the rosy dawn, he went before the sun, and spake thus unto it: \"Thou great star! What would be thy happiness if thou hadst not those for whom thou shinest! \n",
    "For ten years hast thou climbed hither unto my cave: thou wouldsthave wearied of thy light and of the journey, had it not been for me, mine eagle, and my serpent. \n",
    "But we awaited thee every morning, took from thee thine overflow, and blessed thee for it\". \"\"\"\n",
    "\n",
    "predict(lm, THE_BEGGINING);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping up and exporting the code to a `.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import re\n",
    "import requests\n",
    "import bs4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastai.text import TextLMDataBunch, language_model_learner, AWD_LSTM\n",
    "\n",
    "# Amount of sentences to use as validation. This is hardcoded for simplicity.\n",
    "N_VAL = 25\n",
    "\n",
    "ZARATHUSTRA_URL = \"https://archive.org/stream/thusspokezarathu00nietuoft/thusspokezarathu00nietuoft_djvu.txt\"\n",
    "\n",
    "ANTI_GOD_SAMPLE=\"God is a non-sense, a spiritual weakness of a degraded society.\"\n",
    "\n",
    "THE_BEGGINING=\"\"\"When Zarathustra was thirty years old, he left his home and the lake of his home, and went into the mountains. There he \n",
    "enjoyed his spirit and his solitude, and for ten years did not weary of it. But at last his heart changed, and rising one \n",
    "morning with the rosy dawn, he went before the sun, and spake thus unto it: \"Thou great star! What would be thy happiness if thou hadst not those for whom thou shinest! \n",
    "For ten years hast thou climbed hither unto my cave: thou wouldsthave wearied of thy light and of the journey, had it not been for me, mine eagle, and my serpent. \n",
    "But we awaited thee every morning, took from thee thine overflow, and blessed thee for it\". \"\"\"\n",
    "\n",
    "SAMPLES = [ANTI_GOD_SAMPLE, THE_BEGGINING]\n",
    "\n",
    "def make_reproducible():\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    seed = 123\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "def get_sentences():\n",
    "    response = requests.get(ZARATHUSTRA_URL)\n",
    "    html = response.content\n",
    "    soup = bs4.BeautifulSoup(html, 'html.parser')\n",
    "    txt_data = soup.pre.contents[0]\n",
    "    pattern = re.compile(r\"\\n\\n\\n[ \\w'\\(\\)]+\\n\\n\", re.MULTILINE)\n",
    "    txt_data = re.sub(pattern, '', txt_data)\n",
    "    lines = txt_data.split(\"\\n\")\n",
    "    \n",
    "    lines = [l for l in lines if len(l) > 1 and not l.isupper() and not l.istitle()]\n",
    "    \n",
    "    # Collapse the \\n (they are breaking sentences in a wide format), \n",
    "    # After collapsing the \\n we got a large string, split it by \".\"\n",
    "    # Add again the . at the end of the lines\n",
    "    sentences = [ f\"{l}.\" for l in (' '.join(lines)).split(\".\")]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def get_lm_databunch(sentences):\n",
    "    \"\"\" Create a `TextLMDataBunch` for list of sentences \"\"\"\n",
    "    train_df = pd.DataFrame({'label': 0, 'text': sentences[:-N_VAL]}) # This is how you specify data for language modeling\n",
    "    valid_df = pd.DataFrame({'label': 0, 'text': sentences[-N_VAL:]})    \n",
    "    return TextLMDataBunch.from_df(path='.', train_df=train_df, valid_df=valid_df, bs=192)\n",
    "\n",
    "def get_trained_lm(sentences):\n",
    "    data_lm = get_lm_databunch(sentences)\n",
    "    learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5)\n",
    "    learn.fit_one_cycle(10, 1e-2)\n",
    "    return learn\n",
    "\n",
    "def predict(lm, sample, n_words=200):\n",
    "    p = lm.predict(sample, n_words=n_words).replace(\" ,\", \",\").replace(\" .\", \"\").replace(\"xxbos\", \"\\n\")\n",
    "    print(p)\n",
    "    return p\n",
    "\n",
    "def show_sample(lm, s):\n",
    "    ss = np.random.choice(s)\n",
    "    print(f\"Input sentence: {ss}\\n\\n\")\n",
    "    print(f\"Generation: {lm.predict(ss, n_words=200)}\")\n",
    "\n",
    "    \n",
    "def run():\n",
    "    make_reproducible()\n",
    "    s = get_sentences()\n",
    "    lm = get_trained_lm(s)\n",
    "    \n",
    "    show_sample(lm, s)\n",
    "    show_sample(lm, s)\n",
    "    \n",
    "    for sample in SAMPLES:\n",
    "        predict(lm, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Making a Nietzsche - 1. Research Journal & ULMFit.ipynb to exp/day_1.py\r\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py \"Making a Nietzsche - 1. Research Journal & ULMFit.ipynb\" \"day_1.py\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
